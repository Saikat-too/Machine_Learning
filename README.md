# Machine Learning/Deep Learning

This repository consists implementation of papers/library in machine learning and deep learning field.

Goal ----->
1. Implement SGD for logistic regression from scratch
2. Compare Ridge vs Lass regression on a synthetic dataset
3. Implement EM for Gaussian Mixture Models(use scipy.stats)
4. Build a random forest from scratch (use sklearn.tree as a reference)
5. Code a kernel SVM (use quadratic programming with cvxopt)
6. Build a 2-layer MLP with backprop from scratch(NumPy only)
7. Reimplement AlexNet in pytorch (simplify layers)
8. Train an LSTM on text generation
9. Implement a transoformer block for masked language modeling
10.Compare SGD vs Adam on CNN(track loss curves)
11.Train a DCGAN on CIFAR-10(use pytorch)
12.Implement SimCLR on MNIST with PyTorch Lightning
13.Fine-tune BERT on a custome dataset
14.Prune a pre-trained ResNet and measure accuracy drop
15.Integrate SAM into a ResNet training loop (PyTorch). Compare convergence vs SGD/Adam
16.Compare NTK for a 3-layer MLP and visualize its evolution during training
17.Implement MoE layer with top-2 routing(use PyTorch). Test on a multilingual translation task
18.Train a SIREN to represent an image or audio signal (no grids!)
19.Implement a simplified DARTS workflow to search for CNN cells on CIFAR-10
20.Train a diffusion model on MNIST with PyTorch
21.Build a Glow inspired flow model for image generation
22.Implement Bayesian layers with Monte Carlo dropout
23.Train an ensemble of 5 ResNets and measure uncertainty on out-of-distribution data
24.Train an EBM to generate CIFAR-10 samples usin Langevin dynamics
25.Fine tune GPT -2 (of LLaMA) on a custom dataset using LoRA(HuggingFace + PyTorch)
26.Implement Chain of Thought with GPT-3/Claude for math word problems(use OpenAI API)
27.Build a RAG system with FAISS(vector DB) with T5 for QA
28.Implement local/stride attention patterns for text generation
29.Train a transformer to discover symbolic equation from data(e.g , F = ma)
30.Train a ResNet with FP16/AMP and benchmark speed vs FP32
31.Split a transformer across multiple GPUs(Pytorch nn.parallel)
32.Quantize a Vit to 8-bit and measure accuracy drop
33.Implement FlashAttention for a transformer blcok(CUDA optional)
34.Train a large model (e.g., GPT-2 XL) using PyTorch FSDP
35.Train a NeRF on a custom 3D scene (use PyTorch3D or Nerfstudio)
36.Implement PPO for RLHF on a text summarization task
37.Train a GAT for node classification on Cora/MolHIV
38.Build a world model for CartPole control using latent imagination
39.Reproduce a simplified protein folding pipeline(use PyRosetta)



Cya........
